from link_tables import apply_links
from common_imports import *

def load_and_prepare_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    df = pd.read_excel(os.path.join(DATA_DIR, "filtered_datasets", "bbOrders_filtered.xlsx"))
    df = apply_links(df)
    
    columns = [
        '–¢–∏–ø–¢–°', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ–ü–∞—Å—Å–∞–∂–∏—Ä–æ–≤', '–ó–∞–∫–∞–∑—á–∏–∫', '–ö–æ–Ω—Ç–∞–∫—Ç–Ω–æ–µ–õ–∏—Ü–æ',
        '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', '–û—Ç–≤–µ—Ç—Å–≤–µ–Ω–Ω—ã–π', '–¢–°', '–í–æ–¥–∏—Ç–µ–ª—å',
        '–ó–∞–≥—Ä—É–∑–∫–∞–ü—É–Ω–∫—Ç', '–†–∞–∑–≥—Ä—É–∑–∫–∞–ü—É–Ω–∫—Ç',
        '–¢–∏–ø–ó–∞–∫–∞–∑–∞', '–¢–∞—Ä–∏—Ñ', '–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è–°—Ç–æ–∏–º–æ—Å—Ç—å', '–¶–µ–Ω–∞–ó–∞–ß–∞—Å',
        '–ú–Ω–æ–≥–æ–¥–Ω–µ–≤–Ω—ã–π', '–ó–∞—Ä—É–±–µ–∂–Ω–∞—è–ü–æ–µ–∑–¥–∫–∞', '–¢—Ä–µ–±—É–µ—Ç—Å—è–¢—É–∞–ª–µ—Ç', '–ë–∞–≥–∞–∂'
    ]
    
    df = df[columns].dropna(subset=['–¢–∏–ø–¢–°', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ–ü–∞—Å—Å–∞–∂–∏—Ä–æ–≤'])
    df['–ú–∞—Ä—à—Ä—É—Ç'] = df['–ó–∞–≥—Ä—É–∑–∫–∞–ü—É–Ω–∫—Ç'].astype(str) + " ‚Üí " + df['–†–∞–∑–≥—Ä—É–∑–∫–∞–ü—É–Ω–∫—Ç'].astype(str)
    df = df.drop(['–ó–∞–≥—Ä—É–∑–∫–∞–ü—É–Ω–∫—Ç', '–†–∞–∑–≥—Ä—É–∑–∫–∞–ü—É–Ω–∫—Ç'], axis=1)
    
    return df

def train_random_forest(df):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ RandomForest"""
    target = '–¢–∏–ø–¢–°'
    features = [col for col in df.columns if col != target]
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    df_encoded = df.copy()
    encoders = {}
    for col in df_encoded.columns:
        if df_encoded[col].dtype == 'object':
            le = LabelEncoder()
            df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
            encoders[col] = le
    
    X = df_encoded[features]
    y = df_encoded[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    rf = RandomForestClassifier(**RF_PARAMS)
    rf.fit(X_train, y_train)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
    top_10 = importances.head(10).index.tolist()
    
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    X_train_top = X_train[top_10]
    X_test_top = X_test[top_10]
    rf.fit(X_train_top, y_train)
    y_pred_rf = rf.predict(X_test_top)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    logger.info("\nRandomForest Results:")
    logger.info(f"Accuracy: {accuracy_score(y_test, y_pred_rf)}")
    logger.info("\nClassification Report:")
    logger.info(classification_report(y_test, y_pred_rf))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plt.figure(figsize=(10, 6))
    sns.barplot(x=importances.head(10).values, y=importances.head(10).index)
    plt.title("–¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (RandomForest)")
    plt.xlabel("–í–∞–∂–Ω–æ—Å—Ç—å")
    plt.tight_layout()
    plt.grid(True)
    plt.savefig(os.path.join(OUTPUTS_DIR, "feature_importance_rf.png"))
    plt.close()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤
    joblib.dump(rf, os.path.join(MODELS_DIR, "model_typeTS_rf.pkl"))
    joblib.dump(encoders, os.path.join(MODELS_DIR, "encoders.pkl"))
    
    return rf, encoders

def train_catboost(df):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ CatBoost"""
    target = '–¢–∏–ø–¢–°'
    features = [col for col in df.columns if col != target]
    
    X = df[features]
    y = df[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    cat_features = X.select_dtypes(include=['object']).columns.tolist()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    train_pool = Pool(X_train, y_train, cat_features=cat_features)
    test_pool = Pool(X_test, y_test, cat_features=cat_features)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = CatBoostClassifier(**CATBOOST_PARAMS)
    model.fit(train_pool)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    y_pred = model.predict(test_pool)
    logger.info("\nCatBoost Results:")
    logger.info(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    logger.info("\nClassification Report:")
    logger.info(classification_report(y_test, y_pred))
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = model.get_feature_importance(prettified=True)
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importances', y='Feature Id', data=feature_importance.head(10))
    plt.title("–¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (CatBoost)")
    plt.tight_layout()
    plt.grid(True)
    plt.savefig(os.path.join(OUTPUTS_DIR, "feature_importance_catboost.png"))
    plt.close()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.save_model(os.path.join(MODELS_DIR, "catboost_typeTS_model.cbm"))
    
    return model

def analyze_relationships(df):
    """–ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("\n--- üìé –ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π ---")
    
    # –°–≤—è–∑—å –º–µ–∂–¥—É –ó–∞–∫–∞–∑—á–∏–∫–æ–º –∏ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π
    cross1 = pd.crosstab(df['–ó–∞–∫–∞–∑—á–∏–∫'], df['–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'])
    logger.info("\n–ó–∞–∫–∞–∑—á–∏–∫ √ó –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è:")
    logger.info(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–æ–∫: {len(cross1.stack())}")
    logger.info(f"- –°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞ –∑–∞–∫–∞–∑—á–∏–∫–∞: {cross1.astype(bool).sum(axis=1).mean()}")
    
    # –°–≤—è–∑—å –º–µ–∂–¥—É –ó–∞–∫–∞–∑—á–∏–∫–æ–º –∏ –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–º –ª–∏—Ü–æ–º
    cross2 = pd.crosstab(df['–ó–∞–∫–∞–∑—á–∏–∫'], df['–ö–æ–Ω—Ç–∞–∫—Ç–Ω–æ–µ–õ–∏—Ü–æ'])
    logger.info("\n–ó–∞–∫–∞–∑—á–∏–∫ √ó –ö–æ–Ω—Ç–∞–∫—Ç–Ω–æ–µ–õ–∏—Ü–æ:")
    logger.info(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–æ–∫: {len(cross2.stack())}")
    logger.info(f"- –°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö –ª–∏—Ü –Ω–∞ –∑–∞–∫–∞–∑—á–∏–∫–∞: {cross2.astype(bool).sum(axis=1).mean()}")
    
    # –í–æ–¥–∏—Ç–µ–ª—å ‚Üî –¢–°
    vod_tts = df.groupby('–í–æ–¥–∏—Ç–µ–ª—å')['–¢–°'].nunique().sort_values(ascending=False)
    logger.info("\n–í–æ–¥–∏—Ç–µ–ª—å √ó –¢–°:")
    logger.info(vod_tts.describe())
    
    plt.figure(figsize=(8, 4))
    sns.histplot(vod_tts, bins=20)
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¢–° –Ω–∞ –æ–¥–Ω–æ–≥–æ –≤–æ–¥–∏—Ç–µ–ª—è")
    plt.xlabel("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¢–°")
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUTS_DIR, "driver_ts_distribution.png"))
    plt.close()
    
    # –¢–∏–ø–¢–° ‚Üî –¢–°
    tts_typets = df.groupby('–¢–°')['–¢–∏–ø–¢–°'].nunique().sort_values(ascending=False)
    logger.info("\n–¢–° √ó –¢–∏–ø–¢–°:")
    logger.info(tts_typets.describe())
    
    plt.figure(figsize=(8, 4))
    sns.histplot(tts_typets, bins=10)
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¢–∏–ø–¢–° –Ω–∞ –æ–¥–Ω—É –¢–°")
    plt.xlabel("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¢–∏–ø–¢–°")
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUTS_DIR, "ts_typets_distribution.png"))
    plt.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("=== –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = load_and_prepare_data()
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    rf_model, encoders = train_random_forest(df)
    catboost_model = train_catboost(df)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π
    analyze_relationships(df)
    
    logger.info("\n=== –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ===")
    logger.info("–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ outputs/models/")
    logger.info("–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ outputs/")

if __name__ == "__main__":
    main()

